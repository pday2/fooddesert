---
title: "Food Deserts - Unsupervised Learning Analysis"
date: "20-06-2022"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r imports, include=FALSE}
library("factoextra")
library("tidyverse")
library("readxl")
library("dplyr")
library("data.table")
```

# **Assignment 2: Food Deserts - Unsupervised Analysis**

Group members: Christian Marcelo Chafla Bastidas r0874332, Anja DeriÄ‡ r0873512, Theodore Gautier r0814273, Peter Day r0866276

Loading the data

```{r loadData}
edu <- read.csv('./education/edu.csv')
groc <- read.csv('./grocery/grocery.csv')
health <- read.csv('./health/health.csv')
inc <- read.csv('./income/income_cleaned.csv')
race <- read.csv('./race/race.csv')



#Add zipcode information to search for state or county
zip_county <- read.csv(file = 'zip_county.csv')

#New data frame containing zcta, state and county information
zip_county <-zip_county[c(1,7,8)]
names(zip_county)[names(zip_county) == "zip"] <- "zcta"
zip_county$zcta <-as.numeric((zip_county$zcta))

```

Creation of additional variables

```{r clean}
#creation of a new percentage of white variable and data frame 
#containing percent and pop
race["per_white"] <- race[3]/race[2]
racew <- race[c(1,2,10)]

#creation of new variable looking at the percentage of people over the age of 25 who did not obtain a High School diploma
education <- edu[c(1,6,7)]
education['gt25_lt_hs'] <- education[2]+education[3]
educa <- education[c(1,4)]

```

Putting all the data together in one data frame

```{r}
#putting all tables together
df_list <- list(educa,groc,health,inc,racew,zip_county)
test <- df_list %>% reduce(inner_join , by="zcta")
head(test)


```

```{r ,include=FALSE}
#checking for na's and imputing them 
sum(is.na(test))
which(is.na(test), arr.ind=TRUE)
test$grocery_per1k[is.na(test$grocery_per1k)] <- median(test$grocery_per1k, na.rm = T) 
sum(is.na(test))
```

Making a new data frame with only the relevant information and removing duplicate variable rows.

```{r}
#retaining only important info
dat <- test[-c(1,3,4,6,7,9:11)]

#creationg of a new variable for pop density
dat["density pop/sqMile"] <- dat[10]/dat[2]

#remove area  
dat <- dat[-2]
```

# Analysis

## By State

Aggregating the mean variables for each state. We lose a lot of info from the individual observations but this helps us have a general look at the states

```{r warning = FALSE }
# Removing county name and summing the mean for each state with the weight being the total population of each data point
dat_state <- dat[-12]

#aggregating the values with population as weights
dat_table <- data.table(dat_state)
mean_state <- data.frame(dat_table[, lapply(.SD, weighted.mean, w=.SD$tot_pop), by=state])

#removing the population variable
mean_state <- mean_state[,-10]

#Changing the rowname to the state
rownames(mean_state) <- mean_state[,1]
mean_state <- mean_state[,-1]
head(mean_state)

#centering and scaling
z_mean_state<-scale(mean_state,scale=TRUE,center = T) 
```

Once our data is scaled and centered we can start to look for patterns in our data. We will look at attempting to find clusters between the states using a k-means and hierchical clustering algorithms.

### K-means clustering

The first step in this process is to find the optimal number of clusters to use. This hyperparameter can be found by iterating over the data for a particular number of clusters, in this case we look at clusters between 1-20 with each iteration being done 25 times per cluster number. After this we will look at how the within sums of squares (wss) will decrease for each additional cluster.

```{r}

wss = list() #List to store the wss per cluster solution

#looping over the data frame 20 times for each cluster number
for (i in 1:20) {
  km.out = kmeans(z_mean_state, centers=i, nstart=25)
  wss[[i]]=tibble(k=i, ss=km.out$tot.withinss)
} 

#store the obtained wss in a list
wss = bind_rows(wss)

```

We then make a plot comparing the number of clusters with the wss to choose the optimal amount of clusters.

```{r}
#Check optimal number of cluster 
ggplot(wss, aes(x = k, y = ss))+
  geom_line()+
  geom_point()+
  xlab("Number of Clusters")+
  ylab("Within groups sum of squares")
```

The plot shows does not have a nice elbow shape for us to be able to choose a specific cluster number but we decide to choose clusters as this reduces the wss enough while keeping the number of clusters low.

Now that we have our hyperparameter we can rerun the algorithm on only clusters with 50 different k means starting points

```{r}
set.seed(123) #to start from same seed number


km.res = kmeans(z_mean_state, 4, nstart = 50)
print(km.res)
```

Here we can see the means for each clusters on the variables in our data frame and which state belongs to each cluster. To have a more visual understanding we plot this solution on a 2 dimensional plane.

Plotting the clusters on a 2 dimensional plane

```{r}
#Plotting the solution using fviz
fviz_cluster(km.res, z_mean_state, ellipse.type = "norm")
```

Our first immediate observation is the small first cluster composed only of the states of DC and NY which stand out far out of the rest. This cluster has very high mean of grocery per square mile ( mean of 4.25 standard deviations) while also having the lowest means of obesity and cancer. This cluster also has the highest mean income. Not surprisingly it also has the highest mean density. The other 3 clusters do not separate the states nicely. The 3rd cluster does have the highest obesity means while also having the lowest amount of grocery per sq mile. Additionally this cluster has the highest rates of depression and CHD(?) while also having the lowest density and mean income of all the clusters.

### Hierarchical clustering

Our second method is to look at hierarchical clustering to see if we can come up with more distinct clusters.

```{r}
#Hierchical clustering using Hcut algo
hc.res = hcut(z_mean_state, k=4, hc_method = "complete")

```

We will use the same amount of clusters as found in our previous clustering algorithm to be able to compare the two. With hierarchical clustering we can easily plot the dendrogram to have a nice visualization of the clusters.

```{r}
plot(hc.res, cex = 0.5)

rect.hclust(hc.res,  k = 4, # k is used to specify the number of clusters
            border = "red"
)
```

We see that with the 4 cluster solutions we get quite different ones than with the kmeans algorithm. We get one giant cluster and then three smaller ones with NY being its own cluster. Visualizing this solution on a 2d plane we get the following results:

```{r}
#visualisation of the solution
fviz_cluster(hc.res, ellipse.type = "convex")
```


### PCA + PCA clustering

We also try to use principal component analysis (PCA) to reduce the dimensionality of our data.

```{r}
pca = prcomp(z_mean_state)

```

Looking at the percentage of variation explained by each principal component

```{r}
# creating the pca object
pca = prcomp(z_mean_state)
# Eigenvalues to look at the percentage explained by each principal component
eig.val <- get_eigenvalue(pca)
fviz_eig(pca)
```

Plotting the solution with the first two principal components on a biplot :

```{r}
#Biplot of dimensions
fviz_pca_biplot(pca)

```

We see that density and grocery per sq mile are very close together and go in the same direction. We also observe that most of the health related variables go opposite of the mean income variable which would imply that this is a more important factor to understand health problems. As this analysis was done at a state level, it makes sense that food deserts are not represented much by this pca.

This is why we now focus on the counties in a single state.

```{r, include=FALSE}

d5 = pca$x[, c("PC1", "PC2")]

```

```{r, include=FALSE}

wss = list()
for (i in 1:15) {
  km.out = kmeans(d5, centers = i, nstart = 20)
  wss[[i]] = tibble(k=i, ss=km.out$tot.withinss)
}
wss = bind_rows(wss)

# Plot sum of squares vs. number of clusters
ggplot(wss, aes(x=k, y=ss)) + geom_line() + 
  xlab("Number of Clusters") + 
  ylab("Within groups sum of squares")
```

```{r, include=FALSE}
 
set.seed(123)
km.res_5 <- kmeans(d5, 5, nstart = 25)
fviz_cluster(km.res_5, d5, ellipse.type = "norm")
```

## By County for a specific state

To get a better understanding of how food deserts work in specific states, we first sorted the data by grocery store per square mile and by descending population.

```{r}
#Sorting the date by grocery and population
ordered  <- dat[order(dat$grocery_persqmile, -dat$tot_pop),]

#looking at the number of times each state appears in the dataset
nState <- ordered %>% count(ordered$state,sort = TRUE)
names(nState)[1] <- "state"
```

Next we took the first ordered 10,000 data points to look where most of the food deserts were located

```{r}
ordered_low <- ordered[1:10000,]

#counting the number of appearances of each state
nLow <- ordered_low %>% count(ordered_low$state,sort = TRUE)
names(nLow)[1] <- "state"

#
nTot <- merge(nLow,nState, by="state")
nTot["%lowFoodAccess"] <- nTot$n.x/nTot$n.y
nTot_ordered <- nTot[order(-nTot$'%lowFoodAccess'),]

nTot_ordered
```

We can observe that Wyoming, Alaska and Montana have a very high percentage of entries in the lowest 10,000. Thus we decide to focus our research on WY to see if we can differentiate between the counties in Wyoming.

```{r}
#Selecting the datapoints belonging in Wyoming
desert_WY <- subset(ordered, ordered$state=="WY")
desert_WY <- desert_WY[-11]

#Aggregating the mean variables for each county with the weight being the population
my_dt <- data.table(desert_WY)
df <- data.frame(my_dt[, lapply(.SD, weighted.mean, w=.SD$tot_pop), by=county])
df <- df[,-10]

#changing the row names to the county names and removing the total pop variable
rownames(df) <- df[,1]
df <- df[,-1]
head(df)

#Scaling our data
#centering and scaling
z_mean_county<-scale(df,scale=TRUE) 

```

Now that we have the aggregate counties we can do our unsupervised analysis.

### Kmeans Clustering

```{r}
#k-means clustering
#Store within sum of squares (wss) for 1 to 15 clusters

wss = list() #List to store the wss per cluster solution

for (i in 1:15) {
  km.out = kmeans(z_mean_county, centers=i, nstart=25)
  wss[[i]]=tibble(k=i, ss=km.out$tot.withinss)
} #loop for 15 clusters


wss = bind_rows(wss) #store in list

```

Visualizing the optimal amount of clusters

```{r}
#= Optimal number of clusters 
#= number of clusters that decreases the variability maintaining the highest number of clusters

ggplot(wss, aes(x = k, y = ss))+
  geom_line()+
  geom_point()+
  xlab("Number of Clusters")+
  ylab("Within groups sum of squares")
```

We see that once again the optimal number of clusters is not straight forward as we do not see a clear elbow shape until the end. We decide to try to create 5 clusters:

```{r}
set.seed(123) #to start from same seed number

km.res = kmeans(z_mean_county, 4, nstart = 25)

#Chicago and oak park large outliers, blue cluster are 
fviz_cluster(km.res, z_mean_county, ellipse.type = "norm")
#looking at the centers of each cluster
km.res$centers
```

The clusters do not allow us to find very interesting information. We do see that Teton County, which is one of the most concentrated counties in the US in terms of Wealth.

### PCA

We will try to perform a pca on the counties in Wyoming to better understand the relationship between the variables

```{r}
############################PCA
#PCA + scaling to z-scores
pca = prcomp(z_mean_county)

# Eigenvalues to check how much of the variation is explained by each principal component

fviz_eig(pca)


```

We see that the first three principal components can explain 80% of the variation in our data.

Plotting the first two principal components on a biplot we get this solution :

```{r}
#Biplot of dimensions
fviz_pca_biplot(pca,label = "var")
       
```

We observe that once again mean income is and health variables such as depression and obesity are very negatively correlated. When we focused on the state of Wyoming we see that counties that have less grocery stores per square mile also seem to have much higher counts of cardiovascular heart diseases (CHD) and also higher chances of depression and obesity. We also observe that depression and obesity appear more in counties with higher proportions of persons without a high school degree.

```{r, include=FALSE}
#proportion of variance explained by components
print("Proportion of variance explained:")
prop_var = tibble(pc=1:10,
                  var=pca$sdev^2 / sum(pca$sdev^2))
prop_var  

ggplot(prop_var,
       aes(x = pc, y = var))+
  geom_col(width=0.3)+
  scale_y_continuous(limits = c(0,1))+
  theme_bw()+
  labs(x = "Principal component", y = "Prop expl var")

```

```{r,include=FALSE}
#Cumulative explained variance
cvar = cumsum(prop_var)
cvar 

ggplot(cvar, aes(x=pc, y=var)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  xlab("Principal component") +
  ylab("Cumulative explained variance")
```
