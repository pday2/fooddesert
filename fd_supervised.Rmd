---
title: "Food Deserts - Supervised Learning Analysis"
date: "20-06-2022"
output:
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r imports, include=FALSE}
library("tidyverse")
library("MASS") # stepAIC
library("dplyr")
library("caret")
library("randomForest")
library("lmtest") # Breush-Pagan
library("skedastic") # White test for heteroskedasticity
library("car") # boxCox, Box-Tidwell
library("xgboost")

if (Sys.info()[1] == "Windows") {
  setwd("C:/Users/peter/My Tresors/Documentsacer/KULeuven/CollAnaBigData/fooddesert-Work")
} else {
  setwd("/home/muddy/Tresors/Documentsacer/KULeuven/CollAnaBigData/fooddesert-Work")   
}
```


# **Assignment 2: Food Desets - Supervised Analysis**

Group members: Christian Marcelo Chafla Bastidas r0874332, Anja DeriÄ‡ r0873512,
Theodore Gautier r0814273, Peter Day r0866276 

# Overview

1. Research question

2. Research design

3. Analysis

4. Results

5. Conclusion

6. Elevator pitch


# 1. Research question

Is number of grocery stores per square mile related to obesity when controlling for a number of other variables?

This is relevant because, it shows heterogeneous distribution of grocery stores can lead to a number of health problems and disproportionately effects poor, less-educated and black people.

# 2. Design

# 3. Analysis

Step 1: Data sources:

Grocery store data:
[National Neighborhood Data Archive (NaNDA): Grocery Stores by ZIP Code Tabulation Area, United States, 2003-2017](https://www.openicpsr.org/openicpsr/project/123042/version/V1/view)

Demographic data:
[US Census: American Community Survey (ACS)](https://www.census.gov/programs-surveys/acs/)

Health data:
[US CDC: PLACES: Local Data for Better Health, ZCTA Data 2021 release ](https://chronicdata.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-ZCTA-Data-2021/qnzd-25i4)



```{r loadData}
grocery <- read.csv('./grocery/grocery.csv')
health <- read.csv('./health/health.csv')
race <- read.csv('./race/race.csv')
income <- read.csv('./income/income_cleaned.csv')
edu <- read.csv('./education/edu.csv')
```

# Join, clean, variable selection part 1

```{r clean}
# data has all variables, dat will be used for analysis
data <- inner_join(grocery, health, by='zcta')
data <- inner_join(data, race, by='zcta')
data <- inner_join(data, income, by='zcta')
data <- inner_join(data, edu, by='zcta')
# found an na in grocery_per1k - imputed value with median
sum(is.na(data))
which(is.na(data), arr.ind=TRUE)
data$grocery_per1k[is.na(data$grocery_per1k)] <- median(data$grocery_per1k, na.rm = T) 
sum(is.na(data))
# remove zcta, year, Year and geolocation (lat&long) for supervised learning
dat <- data[-c(1,2,8,9)]
# initial linear regressions suggested that some of the age
# ranges are not linearly associated with obesity
dat <- dat[-c(21,22,23,24,25,26,27,28,29,30,31,32,33)]
# and mixed race seems to have problems... multicollinearity?
dat <- dat[-19]
# Removing population stats due to multicollinearity 
dat <- dat[-c(1,6,12)]
```

# Some interesting zip codes

```{r zips}
summary(data$OBESITY)
summary(data$grocery_persqmile)
summary(data$grocery_count)
histogram(data$grocery_count)
# zcta = 10*** = NYC
# Lawrence, Massachusetts 01841
# Boston, Massachusetts 02128
# Hartford, Connecticut 06106
data %>% filter(grocery_count>50)
```


# Train/Validation/Test split

```{r splits}
# Train/Validation/Test split
set.seed(33)
sample <- sample(c(TRUE, FALSE), nrow(dat), replace=TRUE, prob=c(0.6,0.4))
train  <- dat[sample, ]
testvalid   <- dat[!sample, ]
validtest <- sample(c(TRUE, FALSE), nrow(testvalid), replace=TRUE, prob=c(0.5,0.5))
valid <- testvalid[validtest, ]
test <- testvalid[!validtest, ]


# Obesity is variable 5
X_train <- train[-5]
y_train <- train[5]
X_valid <- valid[-5]
y_valid <- valid[5]
X_test <- test[-5]
y_test <- test[5]
```

## Histogram of Obesity
```{r}
# maybe skews left?
histogram(train$OBESITY)
```

## Obesity vs Grocery Stores per square mile
```{r}
plot(train$grocery_persqmile, train$OBESITY)
abline(lm(train$OBESITY~train$grocery_persqmile))
```



# Variable Selection - Two Methods

```{r stepAIC}
fit.full <- lm(OBESITY ~ ., data=train)
stepAIC(fit.full, direction = "backward")
# Backward step selection removes:
#   white, DEPRESSION

```

```{r lm}
# Using results from backward step to fit OLS model
data.step <- train[-c(6,10)]
lm.fitstep <- lm(OBESITY ~ ., data = data.step)
summary(lm.fitstep)
```


```{r FTest}
# Based on F-value
dropterm(fit.full, test = "F")
fit1 <- update(fit.full, ~ . - white)
dropterm(fit1, sorted = TRUE, test = "F")

fit2 <- update(fit1, ~ . - DEPRESSION)
dropterm(fit2, sorted = TRUE, test = "F")

# grocery_per_1k not signif
fit3 <- update(fit2, ~ . - grocery_per1k)
dropterm(fit3, sorted = TRUE, test = "F")


summary(fit3)
```

# Diagnostics on fit 3
```{r diagnostics}
pred = predict(fit3)
sres = stdres(fit3)
student = rstandard(fit3)
# Variance looks fairly stable
plot(pred, sres, pch=16); abline(h=0)
plot(pred, student, pch=16); abline(h=0)
# Variance increases with OBS but not wider/narrower spread
plot(train$OBESITY, sres, pch=16); abline(h=0)
```

```{r residualPlot}
# residual values increase with obesity, though variance doesn't change much
plot(sres, xlab = "Index", ylab = "Standardized residual", ylim = c(-4.0,4.0))
abline(h = -3.0, lty = 2)
abline(h = 3.0, lty = 2)
```

```{r diagnostics2}
boxplot(student, pch=16)
# that's a lot of potential outliers? but maybe ok for 17k obs?

qqnorm(student, pch=16); qqline(student)
# May not be normal at the ends, middle section looks good

# Shapiro-Wilk test for normality
# H0: model errors are normally distributed
# !must have 5000 or fewer observations! we have 17k in training
#shapiro.test(student)


# Breusch-Pagan - test for Homoskedasticity
# H0: residuals are homoskedastic
bptest(fit3, studentize=F)
# BP = 840.07, df = 20, p-value < 2.2e-16
# Reject H0 in favor of H1 - residuals are heteroskedastic

boxcox(fit3, lambda=seq(-2, 2, 0.1), plotit = T)
# lambda ~ 1 => no transformation

# Multicollinearity?
multico = solve(cor(train[, -5]))
summary(multico)
# Now, no values above 10! 
# Multicollinearity fixed by removing population variables
```

```{r WLS, include=FALSE}
# Weighted Least Squares Analysis
datafit3 <- train[-c(3,6,10)]
# Fit using OLS
fit <- lm(OBESITY~., data = datafit3)
# Calculate residuals
resid <- residuals(fit)
# regress sq or abs resids on regressors from OLS
# abs(resid) estimates sigma
# resid^2 estimates sigma^2
stdev <- lm(abs(resid)~., data = datafit3[-4])
summary(stdev)
# Calculate weights from estimated Std Dev w=1/s^2
weights <- 1/stdev$fitted^2
# Fit regression with weights
wls <- lm(OBESITY~., data = datafit3, weights=weights)
summary(wls)

# Breusch-Pagan test - p<0.001, still heteroskedastic
bptest(wls, studentize=F)
wstudent = rstandard(wls)
qqnorm(wstudent, pch=16); qqline(wstudent)

# These will still show heteroskedasticity
plot(datafit3$OBESITY,resid(wls))
# With weights, heteroskedasticity should now be gone
plot(datafit3$OBESITY,resid(wls)*sqrt(weights),ylab="Weighted residuals")

# Fitted/Predicted vs residuals
wpred = predict(wls)
wsres = stdres(wls)
wstudent = rstandard(wls)
plot(wpred, wsres, pch=16); abline(h=0)
plot(wpred, wstudent, pch=16); abline(h=0)

skedastic::white_lm(fit3)
skedastic::white_lm(wls)

```


The fitted value versus residual plot shows that there may not be much concern with heteroskedasticity. However, since the Breusch-Pagan test rejected homoskedasticty (as did the White test) , We fit a weighted least squares model, which did not improve the situation as far as Breusch-Pagan or White are concerned. We thus will stick with OLS as done in fit3 for ease of interpretability.

```{r lm.F}
# Using results from manual F-Test variable selection to fit OLS model
data.F <- train[-c(3,6,10)]
lm.fitF <- lm(OBESITY ~ ., data = data.F)
summary(lm.fitF)
```

# XGBoost

```{r xgboost}
# XGBoost is an ensemble method, similar to Random Forest
# Except not random, tree N+1 is based on the loss from tree N
# https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html
# XGB Params: https://xgboost.readthedocs.io/en/latest/parameter.html?highlight=objective
xgbtrain = xgb.DMatrix(as.matrix(sapply(X_train, as.numeric)), label=y_train$OBESITY)
xgbvalid = xgb.DMatrix(as.matrix(sapply(X_valid, as.numeric)), label=y_valid$OBESITY)
watchlist <- list(train=xgbtrain, test=xgbvalid)

# max.depth = 4 and nrounds=10 yielded minimum test-rmse
xgb.fit <- xgb.train(data=xgbtrain, max.depth = 4, eta = 1, nthread = 2, nrounds = 10, 
                   watchlist=watchlist, objective = "reg:squarederror")
# linear boosting - not as good
lin.fit <- xgb.train(data=xgbtrain, booster = "gblinear", nthread = 2, nrounds=10, 
                 watchlist=watchlist, objective = "reg:squarederror")
summary(xgb.fit)

# Plot of variable importance
# Gain - is the improvement in accuracy brought by a feature to the branches it is on
# Cover - measures the relative quantity of observations concerned by a feature
# Frequency - is a simpler way to measure the Gain. It just counts the number of
#   times a feature is used in all generated trees
#   !You should not use it (unless you know why you want to use it)!
importance_matrix <- xgb.importance(model = xgb.fit)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
```

xgboost's gain shows that percent of the population of age 45-65 with a bachelor's degree or greater, mean income,  percent of population with chronic heart disease, percent of population which is black and percent of population with cancer are the five most important variables in determining percent of population with obesity. Number of grocery stores per square mile is the eigth most important variable.

```{r xgboost2}
# View model trees
# with depth=5 the tree ends up having 608 nodes/leaves
# Commented out since output isn't helpful - >600 lines describing the tree
#xgb.dump(xgb.fit, with_stats = TRUE)


# For example tree with depth = 2... plotting a 5 level tree is not helpful
# With this many variables, its still not visible
xgb.fit2 <- xgb.train(data=xgbtrain, max.depth = 4, eta = 1, nthread = 2, nrounds = 10, 
                     watchlist=watchlist, objective = "reg:squarederror")
xgb.plot.tree(model = xgb.fit2, trees=0) # just from 0th root node
```

# Compare Models - RMSE

```{r}
# Compare RMSE of linear and XGB Models
xgbtest = xgb.DMatrix(as.matrix(sapply(X_test, as.numeric)), label=y_test$OBESITY)

linpred <- predict.lm(fit3, test)
xgbpred <- predict(xgb.fit, xgbtest)

# RMSE of linear model on test set
sqrt(mean((test$OBESITY - linpred)^2))

# RMSE of XGBoost model on test set
sqrt(mean((test$OBESITY - xgbpred)^2))
```


# Supervised Analysis Conclusion

When controlling for variables such as income, race, education and various illnesses, we have found a negative association between the number of grocery stores per square mile in a zip code area and percent of inhabitants who are obese. For every increase of one grocery store per square mile, obesity decreases by 0.0867%. We found some other interesting associations with obesity. As the percentage of inhabitants who are black increases, so does obesity. Obesity decreases with income and education. 

Using xgboost to look at non-linear associations with obesity, we found that education (percent of inhabitants between 45 and 65 years of age with a bachelors or greater), mean income and being black are important variables in determining obesity. Number of grocery stores per square mile, to a lesser degree, is also important to explaining obesity.

With a RMSE of 2.986 our linear model, fit3, outperformed the XGBoost model, which had a RMSE of 3.034, on the test data. 


# Grocery Store Data
Project Citation: 
Finlay, Jessica, Li, Mao, Esposito, Michael, Gomez-Lopez, Iris, Khan, Anam, Clarke, Philippa, and Chenoweth, Megan. National Neighborhood Data Archive (NaNDA): Grocery Stores by ZIP Code Tabulation Area, United States, 2003-2017. Ann Arbor, MI: Inter-university Consortium for Political and Social Research [distributor], 2020-10-01. https://doi.org/10.3886/E123042V1

