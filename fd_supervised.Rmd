---
title: "Food Deserts - Supervised Learning Analysis"
date: "20-06-2022"
output:
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r imports, include=FALSE}
library("tidyverse")
library("MASS") # stepAIC
library("dplyr")
library("caret")
library("randomForest")
library("xgboost")

if (Sys.info()[1] == "Windows") {
  setwd("C:/Users/peter/My Tresors/Documentsacer/KULeuven/CollAnaBigData/fooddesert-Work")
} else {
  setwd("/home/muddy/Tresors/Documentsacer/KULeuven/CollAnaBigData/fooddesert-Work")   
}
```


# **Assignment 2: Food Desets - Supervised Analysis**

Group members: Christian Marcelo Chafla Bastidas r0874332, Anja DeriÄ‡ r0873512,
Theodore Gautier r0814273, Peter Day r0866276 

# Overview

1. Research question

2. Research design

3. Analysis

4. Results

5. Conclusion

6. Elevator pitch


# 1. Research question

Our research question reads as follows: Is number of grocery stores per square mile related to obesity when controlling for a number of other variables?

<spoiler-alert>Yes</spoiler-alert>

This is relevant because, it shows heterogeneous distribution of grocery stores can lead to a number of health problems and disproportionately effects the poor and less-educated.

# 2. Design

# 3. Analysis

Step 1: We import our data. We used dataset XXX / We scraped data from XXX

```{r loadData}
grocery <- read.csv('./grocery/grocery.csv')
health <- read.csv('./health/health.csv')
race <- read.csv('./race/race.csv')
income <- read.csv('./income/income_cleaned.csv')
edu <- read.csv('./education/edu.csv')
```

# Join, clean, variable selection part 1

```{r clean}
data <- inner_join(grocery, health, by='zcta')
data <- inner_join(data, race, by='zcta')
data <- inner_join(data, income, by='zcta')
data <- inner_join(data, edu, by='zcta')
# found an na in grocery_per1k - imputed value with median
sum(is.na(data))
which(is.na(data), arr.ind=TRUE)
data$grocery_per1k[is.na(data$grocery_per1k)] <- median(data$grocery_per1k, na.rm = T) 
sum(is.na(data))
# remove zcta, year, Year and geolocation (lat&long) for supervised learning
dat <- data[-c(1,2,8,9)]
# initial linear regressions suggested that some of the age
# ranges are not linearly associated with obesity
dat <- dat[-c(21,22,23,24,25,26,27,28,29,30,31,32,33)]
# and mixed race seems to have problems... multicollinearity?
dat <- dat[-19]
```


# Train/Validation/Test split

```{r splits}
# Train/Validation/Test split
set.seed(33)
sample <- sample(c(TRUE, FALSE), nrow(dat), replace=TRUE, prob=c(0.6,0.4))
train  <- dat[sample, ]
testvalid   <- dat[!sample, ]
validtest <- sample(c(TRUE, FALSE), nrow(testvalid), replace=TRUE, prob=c(0.5,0.5))
valid <- testvalid[validtest, ]
test <- testvalid[!validtest, ]


# Obesity is variable 7
X_train <- train[-7]
y_train <- train[7]
X_valid <- valid[-7]
y_valid <- valid[7]
X_test <- test[-7]
y_test <- test[7]
```

# Variable Selection - Two Methods

```{r stepAIC}
fit.full <- lm(OBESITY ~ ., data=train)
stepAIC(fit.full, direction = "backward")
# Backward step selection removes:
#   pacific_islan, DEPRESSION, asian, grocery_per1k

```

```{r lm}
# Using results from backward step to fit OLS model
data.step <- train[-c(4,8,16,17)]
lm.fitstep <- lm(OBESITY ~ ., data = data.step)
summary(lm.fitstep)
```


```{r FTest}
# Based on F-value
dropterm(fit.full, test = "F")
fit1 <- update(fit.full, ~ . - pacific_islan)
dropterm(fit1, sorted = TRUE, test = "F")

fit2 <- update(fit1, ~ . - DEPRESSION)
dropterm(fit2, sorted = TRUE, test = "F")

fit3 <- update(fit2, ~ . - asian)
dropterm(fit3, sorted = TRUE, test = "F")

fit4 <- update(fit3, ~ . - grocery_per1k)
dropterm(fit4, sorted = TRUE, test = "F")

fit5 <- update(fit4, ~ . - population)
dropterm(fit5, sorted = TRUE, test = "F")

summary(fit5)
```

```{r lm.F}
# Using results from manual F-Test variable selection to fit OLS model
data.F <- train[-c(1,4,8,16,17)]
lm.fitF <- lm(OBESITY ~ ., data = data.F)
summary(lm.fitF)
```

# XGBoost

```{r xgboost}
# XGBoost is an ensemble method, similar to Random Forest
# Except not random, tree N+1 is based on the loss from tree N
# https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html
# XGB Params: https://xgboost.readthedocs.io/en/latest/parameter.html?highlight=objective
xgbtrain = xgb.DMatrix(as.matrix(sapply(X_train, as.numeric)), label=y_train$OBESITY)
xgbvalid = xgb.DMatrix(as.matrix(sapply(X_valid, as.numeric)), label=y_valid$OBESITY)
watchlist <- list(train=xgbtrain, test=xgbvalid)

# max.depth = 5 and nrounds=10 yielded minimum test-rmse
xgb.fit <- xgb.train(data=xgbtrain, max.depth = 5, eta = 1, nthread = 2, nrounds = 10, 
                   watchlist=watchlist, objective = "reg:squarederror")
# linear boosting - not as good
lin.fit <- xgb.train(data=xgbtrain, booster = "gblinear", nthread = 2, nrounds=10, 
                 watchlist=watchlist, objective = "reg:squarederror")
summary(xgb.fit)

# Plot of variable importance
# Gain is the improvement in accuracy brought by a feature to the branches it is on
# Cover measures the relative quantity of observations concerned by a feature
# Frequency is a simpler way to measure the Gain. It just counts the number of
#   times a feature is used in all generated trees
#   !You should not use it (unless you know why you want to use it)!
importance_matrix <- xgb.importance(model = xgb.fit)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
```

xgboost's gain shows that percent of the population of age 45-65 with a bachelor's degree or greater, mean income,  chronic heart disease, percent of population which is black and percent of population with depression are the five most important variables in determining percent of population with obesity. Number of grocery stores per square mile is the seventh most important variable.

```{r xgboost2}
# View model trees
# with depth=5 the tree ends up having 608 nodes/leaves
# Commented out since output isn't helpful - >600 lines describing the tree
#xgb.dump(xgb.fit, with_stats = TRUE)


# For example tree with depth = 2... plotting a 5 level tree is not helpful
# With this many variables, its still not visible
xgb.fit2 <- xgb.train(data=xgbtrain, max.depth = 5, eta = 1, nthread = 2, nrounds = 10, 
                     watchlist=watchlist, objective = "reg:squarederror")
xgb.plot.tree(model = xgb.fit2, trees=0) # just from 0th root node
```


